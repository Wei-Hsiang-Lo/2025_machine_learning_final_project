{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3195a10e",
   "metadata": {},
   "source": [
    "# Fully Automated Chip Design with RL\n",
    "## Overview\n",
    "$\\qquad$ This notebook implements a simple version of fully automated chip design. In this model, It follows the physics laws, thermal, electric and energy conservation. The goal is to predict the distribution of heat on a chip, via supervised-learning (PINNs), and use reinforcement-learning to find the optimal layout of a chip.\n",
    "\n",
    "## Methodology\n",
    "In order to follow the physics laws, I used PINNs to implement this model and define the loss function with three parts:\n",
    "* **Electrical**: The electrical potential $\\tilde{\\phi}$ must satisfy the Laplace equation with conductivity $\\sigma$\n",
    "$$\\mathcal{L}_elec=\\frac{1}{N}\\sum\\|\\nabla\\cdot(\\sigma\\nabla\\tilde{\\nabla}(x_i))\\|^2$$\n",
    "* **Thermal**: The temperature $\\tilde{T}$ is derived from heat source term couples the electrical and thermal via Joule Heating and heat generated by logic gate.\n",
    "* **Global conservation**: To ensure it follows the energy conservation. By enforcing the first law of thermodynamics globally (Total heat generation = Total heat dissipation).\n",
    "\n",
    "By assuming the temperature on the boundary of the chip to be zero, the domain of the chip is on $[-1,1]\\times[-1,1]$ and the heat flux is in steady-state, we can define the Hard-PINN as:\n",
    "$$\\tilde{T}(x,y)=g(x,y)+f(x,y)\\cdot NN(x,y;\\theta)$$,\n",
    "where $NN(x,y;\\theta)$ is the raw network output, $g(x,y)=0$, and $f(x,y)=(1-x)(1+x)(1-y)(1+y)$. By this structure, we can ensure that $\\tilde{T}$ satisfies BCs strictly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ef3bb",
   "metadata": {},
   "source": [
    "## Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350e5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 16:04:17.349933: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 16:04:17.361644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-03 16:04:17.376053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-03 16:04:17.380444: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-03 16:04:17.391273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定浮點數精度為 float64 (物理模擬建議使用雙精度)\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "# =================================\n",
    "# 1. Parameter Definitions\n",
    "# =================================\n",
    "DTYPE = 'float64'\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS_PHASE_1 = 2000   # 只訓練電位\n",
    "EPOCHS_PHASE_2 = 3000   # 只訓練熱 PDE\n",
    "EPOCHS_PHASE_3 = 10000  # 全開 + Global Loss\n",
    "TOTAL_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2 + EPOCHS_PHASE_3\n",
    "\n",
    "BATCH_SIZE_COLLOC = 10000\n",
    "BATCH_SIZE_BOUNDARY = 2000\n",
    "\n",
    "# Learning Rates\n",
    "LR_FAST = 1e-3\n",
    "LR_SLOW = 5e-4\n",
    "\n",
    "# Physical Coefficients\n",
    "SIGMA_ELEC = 10.0\n",
    "K_THERM = 2.0\n",
    "V_DD = 1.0\n",
    "\n",
    "# =================================\n",
    "# 2. Model Builder\n",
    "# =================================\n",
    "def DNN_builder(in_shape=2, out_shape=2, n_hidden_layers=6, neuron_per_layer=64, actfn=\"swish\"):\n",
    "    input_layer = tf.keras.layers.Input(shape=(in_shape,))\n",
    "    hidden = input_layer\n",
    "    for _ in range(n_hidden_layers):\n",
    "        hidden = tf.keras.layers.Dense(neuron_per_layer, activation=actfn)(hidden)\n",
    "    output_layer = tf.keras.layers.Dense(out_shape, activation=None)(hidden)\n",
    "    model = tf.keras.Model(input_layer, output_layer, name=f\"PINN-{n_hidden_layers}layers\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3602d8",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "Uniformly choose the data on the domain, since the prediction will be better for PINNs while the data is uniformly on the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2052b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 3. Data Generator\n",
    "# =================================\n",
    "@tf.function\n",
    "def generate_data():\n",
    "    x = tf.random.uniform((BATCH_SIZE_COLLOC, 1), -1, 1, dtype=DTYPE)\n",
    "    y = tf.random.uniform((BATCH_SIZE_COLLOC, 1), -1, 1, dtype=DTYPE)\n",
    "\n",
    "    n_b = BATCH_SIZE_BOUNDARY\n",
    "    ones = tf.ones((n_b, 1), dtype=DTYPE)\n",
    "    vals = tf.cast(tf.linspace(-1.0, 1.0, n_b)[:, None], DTYPE)\n",
    "\n",
    "    x_r, y_r = ones, vals\n",
    "    x_l, y_l = -ones, vals\n",
    "    x_t, y_t = vals, ones\n",
    "    x_b, y_b = vals, -ones\n",
    "    \n",
    "    return x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01317081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting data point distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 16:04:19.905692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22803 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:37:00.0, compute capability: 7.5\n",
      "2025-12-03 16:04:19.907132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22803 MB memory:  -> device: 1, name: Quadro RTX 6000, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Plotting data point distribution...\")\n",
    "# 生成一批資料\n",
    "x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r = generate_data()\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "    \n",
    "# 1. 畫內部點 (Domain Points) - 藍色點\n",
    "plt.scatter(x.numpy(), y.numpy(), c='blue', s=1, alpha=0.5, label='Interior Domain')\n",
    "    \n",
    "# 2. 畫邊界點 (Boundary Points) - 紅色點\n",
    "# 為了看清楚，我們把四個邊的點合併畫\n",
    "x_bound = tf.concat([x_t, x_b, x_l, x_r], axis=0)\n",
    "y_bound = tf.concat([y_t, y_b, y_l, y_r], axis=0)\n",
    "plt.scatter(x_bound.numpy(), y_bound.numpy(), c='red', s=1, label='Boundaries')\n",
    "    \n",
    "plt.title(f\"Data Distribution\\n(Interior: {len(x)}, Boundary: {len(x_bound)})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.savefig(\"data_distribution.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357670c7",
   "metadata": {},
   "source": [
    "## Hard constraints and the assuming layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a871bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 4. Physics Helper Functions\n",
    "# =================================\n",
    "def hard_constraint_T(x, y):\n",
    "    return (1.0 - x ** 2) * (1.0 - y ** 2)\n",
    "\n",
    "def get_chip_layout_heat(x, y):\n",
    "    q_dyn_1 = 15.0 * tf.exp(-(x**2 + y**2) / (2 * 0.2**2))\n",
    "    q_dyn_2 = 5.0 * tf.exp(-((x**2 + y**2 - 0.5)**2) / (2 * 0.1**2))\n",
    "    return q_dyn_1 + q_dyn_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a3e1c",
   "metadata": {},
   "source": [
    "## Training, validation, and reinforcing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6262911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Coupled Electro-Thermal Simulation ===\n",
      "Device: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15000/15000 [1:05:44<00:00,  3.80ep/s, Ph=Ph3:Global, L_T=1.7e-02, Gen=17.3, Flux=17.4, Err=0.1%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Finished in 3944.34 seconds.\n",
      "\n",
      "=== Validation on Unseen Grid ===\n",
      "Validation Thermal PDE Error: Mean=1.02e-01\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 5. Core Physics Engine (Loss)\n",
    "# =================================\n",
    "@tf.function\n",
    "def compute_physics_loss(model, x, y, x_top, y_top, x_bot, y_bot, x_left, y_left, x_right, y_right):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([x, y])\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            \n",
    "            outputs = model(tf.concat([x, y], axis=1))\n",
    "            phi = outputs[:, 0:1]\n",
    "            T_raw = outputs[:, 1:2]\n",
    "            D_vals = hard_constraint_T(x, y)\n",
    "            T = T_raw * D_vals\n",
    "        \n",
    "        grad_phi_x = tape1.gradient(phi, x)\n",
    "        grad_phi_y = tape1.gradient(phi, y)\n",
    "        grad_T_x = tape1.gradient(T, x)\n",
    "        grad_T_y = tape1.gradient(T, y)\n",
    "        \n",
    "    grad2_phi_xx = tape2.gradient(grad_phi_x, x)\n",
    "    grad2_phi_yy = tape2.gradient(grad_phi_y, y)\n",
    "    grad2_T_xx = tape2.gradient(grad_T_x, x)\n",
    "    grad2_T_yy = tape2.gradient(grad_T_y, y)\n",
    "    \n",
    "    del tape1, tape2\n",
    "\n",
    "    # --- Physics 1: Elec ---\n",
    "    res_elec = SIGMA_ELEC * (grad2_phi_xx + grad2_phi_yy)\n",
    "    loss_elec = tf.reduce_mean(tf.square(res_elec))\n",
    "\n",
    "    # --- Physics 2: Therm ---\n",
    "    J_x = -SIGMA_ELEC * grad_phi_x\n",
    "    J_y = -SIGMA_ELEC * grad_phi_y\n",
    "    Q_joule = (1.0 / SIGMA_ELEC) * (J_x**2 + J_y**2)\n",
    "    Q_logic = get_chip_layout_heat(x, y)\n",
    "    Q_total = Q_joule + Q_logic\n",
    "    \n",
    "    res_therm = K_THERM * (grad2_T_xx + grad2_T_yy) + Q_total\n",
    "    loss_therm = tf.reduce_mean(tf.square(res_therm))\n",
    "\n",
    "    # --- Physics 3: Global ---\n",
    "    total_gen = tf.reduce_mean(Q_total) * 4.0\n",
    "    \n",
    "    def get_boundary_flux(x_b, y_b, nx, ny):\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            t.watch([x_b, y_b])\n",
    "            out = model(tf.concat([x_b, y_b], axis=1))\n",
    "            T_r = out[:, 1:2]\n",
    "            D = hard_constraint_T(x_b, y_b)\n",
    "            T_b = T_r * D\n",
    "        grad_x = t.gradient(T_b, x_b)\n",
    "        grad_y = t.gradient(T_b, y_b)\n",
    "        return -K_THERM * (grad_x * nx + grad_y * ny)\n",
    "\n",
    "    flux_r = tf.reduce_mean(get_boundary_flux(x_right, y_right, 1.0, 0.0)) * 2.0\n",
    "    flux_l = tf.reduce_mean(get_boundary_flux(x_left, y_left, -1.0, 0.0)) * 2.0\n",
    "    flux_t = tf.reduce_mean(get_boundary_flux(x_top, y_top, 0.0, 1.0)) * 2.0\n",
    "    flux_b = tf.reduce_mean(get_boundary_flux(x_bot, y_bot, 0.0, -1.0)) * 2.0\n",
    "    \n",
    "    total_flux_out = flux_r + flux_l + flux_t + flux_b\n",
    "    loss_global = tf.square(total_gen - total_flux_out)\n",
    "\n",
    "    # --- BCs ---\n",
    "    phi_top = model(tf.concat([x_top, y_top], axis=1))[:, 0:1]\n",
    "    phi_bot = model(tf.concat([x_bot, y_bot], axis=1))[:, 0:1]\n",
    "    loss_bc_elec = tf.reduce_mean(tf.square(phi_top - V_DD)) + \\\n",
    "                   tf.reduce_mean(tf.square(phi_bot - 0.0))\n",
    "\n",
    "    return loss_elec, loss_therm, loss_global, loss_bc_elec, total_gen, total_flux_out\n",
    "\n",
    "# =================================\n",
    "# 6. Training Loop (Updated for History)\n",
    "# =================================\n",
    "model = DNN_builder(out_shape=2)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    LR_SLOW, decay_steps=2000, decay_rate=0.95, staircase=True)\n",
    "optimizer_fast = tf.keras.optimizers.Adam(learning_rate=LR_FAST)\n",
    "optimizer_slow = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "print(f\"=== Starting Coupled Electro-Thermal Simulation ===\")\n",
    "print(f\"Device: {tf.config.list_physical_devices('GPU') or 'CPU'}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 字典用於儲存詳細歷史\n",
    "history = {\n",
    "    'total': [], 'elec': [], 'therm': [], 'glob': [], 'bc': [],\n",
    "    'gen_val': [], 'flux_val': []\n",
    "}\n",
    "\n",
    "pbar = tqdm(range(1, TOTAL_EPOCHS + 1), desc=\"Training\", unit=\"ep\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r = generate_data()\n",
    "    \n",
    "    if epoch <= EPOCHS_PHASE_1:\n",
    "        phase = \"Ph1:Elec\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 0.0, 0.0, 20.0\n",
    "        opt = optimizer_fast\n",
    "    elif epoch <= EPOCHS_PHASE_1 + EPOCHS_PHASE_2:\n",
    "        phase = \"Ph2:Therm\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 1.0, 0.0, 20.0\n",
    "        opt = optimizer_fast\n",
    "    else:\n",
    "        phase = \"Ph3:Global\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 1.0, 5.0, 20.0\n",
    "        opt = optimizer_slow\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l_e, l_t, l_g, l_bc, val_gen, val_flux = compute_physics_loss(\n",
    "            model, x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r\n",
    "        )\n",
    "        total_loss = w_e*l_e + w_t*l_t + w_g*l_g + w_bc*l_bc\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # 紀錄數據 (每 10 epochs 記一次)\n",
    "    if epoch % 10 == 0:\n",
    "        history['total'].append(total_loss.numpy())\n",
    "        history['elec'].append(l_e.numpy())\n",
    "        history['therm'].append(l_t.numpy())\n",
    "        history['glob'].append(l_g.numpy())\n",
    "        history['bc'].append(l_bc.numpy())\n",
    "        history['gen_val'].append(val_gen.numpy())\n",
    "        history['flux_val'].append(val_flux.numpy())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        err_p = 0.0\n",
    "        if val_gen > 1e-5:\n",
    "            err_p = abs(val_gen - val_flux) / val_gen * 100\n",
    "            \n",
    "        pbar.set_postfix({\n",
    "            \"Ph\": phase,\n",
    "            \"L_T\": f\"{l_t:.1e}\",\n",
    "            \"Gen\": f\"{val_gen:.1f}\",\n",
    "            \"Flux\": f\"{val_flux:.1f}\",\n",
    "            \"Err\": f\"{err_p:.1f}%\"\n",
    "        })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining Finished in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# =================================\n",
    "# 7. Validation\n",
    "# =================================\n",
    "print(\"\\n=== Validation on Unseen Grid ===\")\n",
    "n_val = 200\n",
    "x_v = tf.cast(tf.linspace(-1.0, 1.0, n_val), DTYPE)\n",
    "y_v = tf.cast(tf.linspace(-1.0, 1.0, n_val), DTYPE)\n",
    "X_val, Y_val = tf.meshgrid(x_v, y_v)\n",
    "x_val_flat = tf.reshape(X_val, [-1, 1])\n",
    "y_val_flat = tf.reshape(Y_val, [-1, 1])\n",
    "\n",
    "def get_residuals(model, x, y):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([x, y])\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            out = model(tf.concat([x, y], axis=1))\n",
    "            phi, T_raw = out[:, 0:1], out[:, 1:2]\n",
    "            T = T_raw * hard_constraint_T(x, y)\n",
    "        gp_x, gp_y = tape1.gradient(phi, x), tape1.gradient(phi, y)\n",
    "        gt_x, gt_y = tape1.gradient(T, x), tape1.gradient(T, y)\n",
    "    g2p_xx = tape2.gradient(gp_x, x)\n",
    "    g2p_yy = tape2.gradient(gp_y, y)\n",
    "    g2t_xx = tape2.gradient(gt_x, x)\n",
    "    g2t_yy = tape2.gradient(gt_y, y)\n",
    "    \n",
    "    res_e = SIGMA_ELEC * (g2p_xx + g2p_yy)\n",
    "    J2 = (SIGMA_ELEC*gp_x)**2 + (SIGMA_ELEC*gp_y)**2\n",
    "    Q = (1.0/SIGMA_ELEC)*J2 + get_chip_layout_heat(x, y)\n",
    "    res_t = K_THERM * (g2t_xx + g2t_yy) + Q\n",
    "    return res_e, res_t\n",
    "\n",
    "res_e_val, res_t_val = get_residuals(model, x_val_flat, y_val_flat)\n",
    "mae_t = tf.reduce_mean(tf.abs(res_t_val))\n",
    "print(f\"Validation Thermal PDE Error: Mean={mae_t:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b68ca",
   "metadata": {},
   "source": [
    "## Generating the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7790c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Advanced Visualization Plots...\n",
      "All plots saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 8. Advanced Visualization (最終修復版)\n",
    "# =================================\n",
    "print(\"Generating Advanced Visualization Plots...\")\n",
    "\n",
    "# --- 1. 準備繪圖資料 ---\n",
    "n_grid = 400\n",
    "x_vals = np.linspace(-1, 1, n_grid)\n",
    "y_vals = np.linspace(-1, 1, n_grid)\n",
    "X_grid, Y_grid = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# [關鍵修正] 準備給 TF 的輸入，必須是 (N, 1)\n",
    "# 但後續計算全部轉回 (N,) 一維陣列以避免廣播錯誤\n",
    "x_flat_input = tf.cast(X_grid.flatten()[:, None], DTYPE)\n",
    "y_flat_input = tf.cast(Y_grid.flatten()[:, None], DTYPE)\n",
    "\n",
    "# --- 2. 預測與物理量計算 ---\n",
    "# Model Prediction\n",
    "out = model(tf.concat([x_flat_input, y_flat_input], axis=1))\n",
    "\n",
    "# [關鍵修正] 全部強制壓扁成 1D array (N,)\n",
    "phi_pred_flat = out[:, 0].numpy().flatten()\n",
    "T_raw_flat = out[:, 1].numpy().flatten()\n",
    "x_flat = X_grid.flatten()\n",
    "y_flat = Y_grid.flatten()\n",
    "\n",
    "# 應用 Hard Constraint (現在大家都是 1D，相乘絕對安全)\n",
    "dist_flat = (1 - x_flat**2) * (1 - y_flat**2)\n",
    "T_pred_flat = T_raw_flat * dist_flat\n",
    "\n",
    "# 重塑回 2D 網格 (400, 400)\n",
    "phi_grid = phi_pred_flat.reshape(n_grid, n_grid)\n",
    "T_grid = T_pred_flat.reshape(n_grid, n_grid)\n",
    "\n",
    "# 計算梯度\n",
    "dy = dx = 2.0 / (n_grid - 1)\n",
    "grad_phi_y, grad_phi_x = np.gradient(phi_grid, dy, dx)\n",
    "grad_T_y, grad_T_x = np.gradient(T_grid, dy, dx)\n",
    "\n",
    "# 計算電流與熱源\n",
    "Jx_grid = -SIGMA_ELEC * grad_phi_x\n",
    "Jy_grid = -SIGMA_ELEC * grad_phi_y\n",
    "J_mag_grid = np.sqrt(Jx_grid**2 + Jy_grid**2)\n",
    "Q_joule_grid = (1.0 / SIGMA_ELEC) * J_mag_grid**2\n",
    "\n",
    "# 計算邏輯熱 (直接用 meshgrid 計算，避免維度問題)\n",
    "Q_logic_tf = get_chip_layout_heat(X_grid, Y_grid)\n",
    "Q_logic_grid = Q_logic_tf.numpy()\n",
    "Q_total_grid = Q_joule_grid + Q_logic_grid\n",
    "\n",
    "# 熱通量\n",
    "qx_grid = -K_THERM * grad_T_x\n",
    "qy_grid = -K_THERM * grad_T_y\n",
    "\n",
    "# --- 3. 開始繪圖 ---\n",
    "\n",
    "# Figure 1: Training Diagnostics\n",
    "# 檢查 history 是否為空\n",
    "if len(history['total']) > 0:\n",
    "    fig1, ax1 = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    steps_per_record = 10 \n",
    "    epochs_idx = np.arange(len(history['total'])) * steps_per_record\n",
    "\n",
    "    ax1[0].semilogy(epochs_idx, history['total'], 'k-', label='Total')\n",
    "    ax1[0].semilogy(epochs_idx, history['elec'], 'b--', label='Elec')\n",
    "    ax1[0].semilogy(epochs_idx, history['therm'], 'r--', label='Therm')\n",
    "    ax1[0].semilogy(epochs_idx, history['glob'], 'g:', label='Global')\n",
    "    ax1[0].axvline(x=EPOCHS_PHASE_1, color='gray', linestyle='--')\n",
    "    ax1[0].axvline(x=EPOCHS_PHASE_1+EPOCHS_PHASE_2, color='gray', linestyle='--')\n",
    "    ax1[0].set_title('Loss History')\n",
    "    ax1[0].legend()\n",
    "    ax1[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax1[1].plot(epochs_idx, history['gen_val'], 'r-', label='Gen (Input)')\n",
    "    ax1[1].plot(epochs_idx, history['flux_val'], 'b--', label='Flux (Output)')\n",
    "    ax1[1].axvline(x=EPOCHS_PHASE_1, color='gray', linestyle='--')\n",
    "    ax1[1].axvline(x=EPOCHS_PHASE_1+EPOCHS_PHASE_2, color='gray', linestyle='--')\n",
    "    ax1[1].set_title('Global Energy Conservation')\n",
    "    ax1[1].legend()\n",
    "    ax1[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('1_training_diagnostics.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Figure 2: Multi-Physics Analysis\n",
    "fig2, ax2 = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Potential & Current\n",
    "c1 = ax2[0].contourf(X_grid, Y_grid, phi_grid, 50, cmap='plasma')\n",
    "plt.colorbar(c1, ax=ax2[0], label='Potential (V)')\n",
    "ax2[0].streamplot(X_grid, Y_grid, Jx_grid, Jy_grid, color='white', linewidth=0.8, density=1.0)\n",
    "ax2[0].set_title('Potential & Current Flow')\n",
    "\n",
    "# Heat Source\n",
    "c2 = ax2[1].contourf(X_grid, Y_grid, Q_total_grid, 50, cmap='inferno')\n",
    "plt.colorbar(c2, ax=ax2[1], label='Heat Source')\n",
    "ax2[1].set_title('Total Heat Source')\n",
    "\n",
    "# Temp & Flux\n",
    "c3 = ax2[2].contourf(X_grid, Y_grid, T_grid, 50, cmap='turbo')\n",
    "plt.colorbar(c3, ax=ax2[2], label='Temp (K)')\n",
    "skip = 25\n",
    "ax2[2].quiver(X_grid[::skip, ::skip], Y_grid[::skip, ::skip], \n",
    "              qx_grid[::skip, ::skip], qy_grid[::skip, ::skip], \n",
    "              color='black', scale=500, width=0.005)\n",
    "ax2[2].set_title('Temperature & Flux')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('2_multiphysics_analysis.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: 1D Slices\n",
    "mid = n_grid // 2\n",
    "fig3, ax3 = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elec Slice\n",
    "ax3[0].plot(x_vals, phi_grid[mid, :], 'b-', label='Phi')\n",
    "ax3[0].set_ylabel('Potential')\n",
    "ax3_twin = ax3[0].twinx()\n",
    "ax3_twin.plot(x_vals, J_mag_grid[mid, :], 'r--', label='|J|')\n",
    "ax3_twin.set_ylabel('|J|', color='r')\n",
    "ax3[0].set_title('1D Slice: Electrical (y=0)')\n",
    "ax3[0].grid(True)\n",
    "\n",
    "# Therm Slice\n",
    "ax3[1].plot(x_vals, T_grid[mid, :], 'k-', label='Temp')\n",
    "ax3[1].set_ylabel('Temp')\n",
    "ax3_twin2 = ax3[1].twinx()\n",
    "ax3_twin2.plot(x_vals, Q_total_grid[mid, :], 'm:', label='Q')\n",
    "ax3_twin2.set_ylabel('Heat Source', color='m')\n",
    "ax3[1].set_title('1D Slice: Thermal (y=0)')\n",
    "ax3[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('3_quantitative_slices.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"All plots saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
