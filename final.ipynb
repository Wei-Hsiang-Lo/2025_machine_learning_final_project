{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "350e5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定浮點數精度為 float64 (物理模擬建議使用雙精度)\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "# =================================\n",
    "# 1. Parameter Definitions\n",
    "# =================================\n",
    "DTYPE = 'float64'\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS_PHASE_1 = 2000   # 只訓練電位\n",
    "EPOCHS_PHASE_2 = 3000   # 只訓練熱 PDE\n",
    "EPOCHS_PHASE_3 = 10000  # 全開 + Global Loss\n",
    "TOTAL_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2 + EPOCHS_PHASE_3\n",
    "\n",
    "BATCH_SIZE_COLLOC = 10000\n",
    "BATCH_SIZE_BOUNDARY = 2000\n",
    "\n",
    "# Learning Rates\n",
    "LR_FAST = 1e-3\n",
    "LR_SLOW = 5e-4\n",
    "\n",
    "# Physical Coefficients\n",
    "SIGMA_ELEC = 10.0\n",
    "K_THERM = 2.0\n",
    "V_DD = 1.0\n",
    "\n",
    "# =================================\n",
    "# 2. Model Builder\n",
    "# =================================\n",
    "def DNN_builder(in_shape=2, out_shape=2, n_hidden_layers=6, neuron_per_layer=64, actfn=\"swish\"):\n",
    "    input_layer = tf.keras.layers.Input(shape=(in_shape,))\n",
    "    hidden = input_layer\n",
    "    for _ in range(n_hidden_layers):\n",
    "        hidden = tf.keras.layers.Dense(neuron_per_layer, activation=actfn)(hidden)\n",
    "    output_layer = tf.keras.layers.Dense(out_shape, activation=None)(hidden)\n",
    "    model = tf.keras.Model(input_layer, output_layer, name=f\"PINN-{n_hidden_layers}layers\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2052b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 3. Data Generator\n",
    "# =================================\n",
    "@tf.function\n",
    "def generate_data():\n",
    "    x = tf.random.uniform((BATCH_SIZE_COLLOC, 1), -1, 1, dtype=DTYPE)\n",
    "    y = tf.random.uniform((BATCH_SIZE_COLLOC, 1), -1, 1, dtype=DTYPE)\n",
    "\n",
    "    n_b = BATCH_SIZE_BOUNDARY\n",
    "    ones = tf.ones((n_b, 1), dtype=DTYPE)\n",
    "    vals = tf.cast(tf.linspace(-1.0, 1.0, n_b)[:, None], DTYPE)\n",
    "\n",
    "    x_r, y_r = ones, vals\n",
    "    x_l, y_l = -ones, vals\n",
    "    x_t, y_t = vals, ones\n",
    "    x_b, y_b = vals, -ones\n",
    "    \n",
    "    return x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01317081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting data point distribution...\n"
     ]
    }
   ],
   "source": [
    "print(\"Plotting data point distribution...\")\n",
    "# 生成一批資料\n",
    "x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r = generate_data()\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "    \n",
    "# 1. 畫內部點 (Domain Points) - 藍色點\n",
    "plt.scatter(x.numpy(), y.numpy(), c='blue', s=1, alpha=0.5, label='Interior Domain')\n",
    "    \n",
    "# 2. 畫邊界點 (Boundary Points) - 紅色點\n",
    "# 為了看清楚，我們把四個邊的點合併畫\n",
    "x_bound = tf.concat([x_t, x_b, x_l, x_r], axis=0)\n",
    "y_bound = tf.concat([y_t, y_b, y_l, y_r], axis=0)\n",
    "plt.scatter(x_bound.numpy(), y_bound.numpy(), c='red', s=1, label='Boundaries')\n",
    "    \n",
    "plt.title(f\"Data Distribution\\n(Interior: {len(x)}, Boundary: {len(x_bound)})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.savefig(\"data_distribution.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a871bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 4. Physics Helper Functions\n",
    "# =================================\n",
    "def hard_constraint_T(x, y):\n",
    "    return (1.0 - x ** 2) * (1.0 - y ** 2)\n",
    "\n",
    "def get_chip_layout_heat(x, y):\n",
    "    q_dyn_1 = 15.0 * tf.exp(-(x**2 + y**2) / (2 * 0.2**2))\n",
    "    q_dyn_2 = 5.0 * tf.exp(-((x**2 + y**2 - 0.5)**2) / (2 * 0.1**2))\n",
    "    return q_dyn_1 + q_dyn_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Coupled Electro-Thermal Simulation ===\n",
      "Device: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 261/15000 [01:32<1:07:02,  3.66ep/s, Ph=Ph1:Elec, L_T=3.2e+01, Gen=17.8, Flux=-1.8, Err=109.9%]"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 5. Core Physics Engine (Loss)\n",
    "# =================================\n",
    "@tf.function\n",
    "def compute_physics_loss(model, x, y, x_top, y_top, x_bot, y_bot, x_left, y_left, x_right, y_right):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([x, y])\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            \n",
    "            outputs = model(tf.concat([x, y], axis=1))\n",
    "            phi = outputs[:, 0:1]\n",
    "            T_raw = outputs[:, 1:2]\n",
    "            D_vals = hard_constraint_T(x, y)\n",
    "            T = T_raw * D_vals\n",
    "        \n",
    "        grad_phi_x = tape1.gradient(phi, x)\n",
    "        grad_phi_y = tape1.gradient(phi, y)\n",
    "        grad_T_x = tape1.gradient(T, x)\n",
    "        grad_T_y = tape1.gradient(T, y)\n",
    "        \n",
    "    grad2_phi_xx = tape2.gradient(grad_phi_x, x)\n",
    "    grad2_phi_yy = tape2.gradient(grad_phi_y, y)\n",
    "    grad2_T_xx = tape2.gradient(grad_T_x, x)\n",
    "    grad2_T_yy = tape2.gradient(grad_T_y, y)\n",
    "    \n",
    "    del tape1, tape2\n",
    "\n",
    "    # --- Physics 1: Elec ---\n",
    "    res_elec = SIGMA_ELEC * (grad2_phi_xx + grad2_phi_yy)\n",
    "    loss_elec = tf.reduce_mean(tf.square(res_elec))\n",
    "\n",
    "    # --- Physics 2: Therm ---\n",
    "    J_x = -SIGMA_ELEC * grad_phi_x\n",
    "    J_y = -SIGMA_ELEC * grad_phi_y\n",
    "    Q_joule = (1.0 / SIGMA_ELEC) * (J_x**2 + J_y**2)\n",
    "    Q_logic = get_chip_layout_heat(x, y)\n",
    "    Q_total = Q_joule + Q_logic\n",
    "    \n",
    "    res_therm = K_THERM * (grad2_T_xx + grad2_T_yy) + Q_total\n",
    "    loss_therm = tf.reduce_mean(tf.square(res_therm))\n",
    "\n",
    "    # --- Physics 3: Global ---\n",
    "    total_gen = tf.reduce_mean(Q_total) * 4.0\n",
    "    \n",
    "    def get_boundary_flux(x_b, y_b, nx, ny):\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            t.watch([x_b, y_b])\n",
    "            out = model(tf.concat([x_b, y_b], axis=1))\n",
    "            T_r = out[:, 1:2]\n",
    "            D = hard_constraint_T(x_b, y_b)\n",
    "            T_b = T_r * D\n",
    "        grad_x = t.gradient(T_b, x_b)\n",
    "        grad_y = t.gradient(T_b, y_b)\n",
    "        return -K_THERM * (grad_x * nx + grad_y * ny)\n",
    "\n",
    "    flux_r = tf.reduce_mean(get_boundary_flux(x_right, y_right, 1.0, 0.0)) * 2.0\n",
    "    flux_l = tf.reduce_mean(get_boundary_flux(x_left, y_left, -1.0, 0.0)) * 2.0\n",
    "    flux_t = tf.reduce_mean(get_boundary_flux(x_top, y_top, 0.0, 1.0)) * 2.0\n",
    "    flux_b = tf.reduce_mean(get_boundary_flux(x_bot, y_bot, 0.0, -1.0)) * 2.0\n",
    "    \n",
    "    total_flux_out = flux_r + flux_l + flux_t + flux_b\n",
    "    loss_global = tf.square(total_gen - total_flux_out)\n",
    "\n",
    "    # --- BCs ---\n",
    "    phi_top = model(tf.concat([x_top, y_top], axis=1))[:, 0:1]\n",
    "    phi_bot = model(tf.concat([x_bot, y_bot], axis=1))[:, 0:1]\n",
    "    loss_bc_elec = tf.reduce_mean(tf.square(phi_top - V_DD)) + \\\n",
    "                   tf.reduce_mean(tf.square(phi_bot - 0.0))\n",
    "\n",
    "    return loss_elec, loss_therm, loss_global, loss_bc_elec, total_gen, total_flux_out\n",
    "\n",
    "# =================================\n",
    "# 6. Training Loop (Updated for History)\n",
    "# =================================\n",
    "model = DNN_builder(out_shape=2)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    LR_SLOW, decay_steps=2000, decay_rate=0.95, staircase=True)\n",
    "optimizer_fast = tf.keras.optimizers.Adam(learning_rate=LR_FAST)\n",
    "optimizer_slow = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "print(f\"=== Starting Coupled Electro-Thermal Simulation ===\")\n",
    "print(f\"Device: {tf.config.list_physical_devices('GPU') or 'CPU'}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 字典用於儲存詳細歷史\n",
    "history = {\n",
    "    'total': [], 'elec': [], 'therm': [], 'glob': [], 'bc': [],\n",
    "    'gen_val': [], 'flux_val': []\n",
    "}\n",
    "\n",
    "pbar = tqdm(range(1, TOTAL_EPOCHS + 1), desc=\"Training\", unit=\"ep\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r = generate_data()\n",
    "    \n",
    "    if epoch <= EPOCHS_PHASE_1:\n",
    "        phase = \"Ph1:Elec\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 0.0, 0.0, 20.0\n",
    "        opt = optimizer_fast\n",
    "    elif epoch <= EPOCHS_PHASE_1 + EPOCHS_PHASE_2:\n",
    "        phase = \"Ph2:Therm\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 1.0, 0.0, 20.0\n",
    "        opt = optimizer_fast\n",
    "    else:\n",
    "        phase = \"Ph3:Global\"\n",
    "        w_e, w_t, w_g, w_bc = 1.0, 1.0, 5.0, 20.0\n",
    "        opt = optimizer_slow\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l_e, l_t, l_g, l_bc, val_gen, val_flux = compute_physics_loss(\n",
    "            model, x, y, x_t, y_t, x_b, y_b, x_l, y_l, x_r, y_r\n",
    "        )\n",
    "        total_loss = w_e*l_e + w_t*l_t + w_g*l_g + w_bc*l_bc\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # 紀錄數據 (每 10 epochs 記一次)\n",
    "    if epoch % 10 == 0:\n",
    "        history['total'].append(total_loss.numpy())\n",
    "        history['elec'].append(l_e.numpy())\n",
    "        history['therm'].append(l_t.numpy())\n",
    "        history['glob'].append(l_g.numpy())\n",
    "        history['bc'].append(l_bc.numpy())\n",
    "        history['gen_val'].append(val_gen.numpy())\n",
    "        history['flux_val'].append(val_flux.numpy())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        err_p = 0.0\n",
    "        if val_gen > 1e-5:\n",
    "            err_p = abs(val_gen - val_flux) / val_gen * 100\n",
    "            \n",
    "        pbar.set_postfix({\n",
    "            \"Ph\": phase,\n",
    "            \"L_T\": f\"{l_t:.1e}\",\n",
    "            \"Gen\": f\"{val_gen:.1f}\",\n",
    "            \"Flux\": f\"{val_flux:.1f}\",\n",
    "            \"Err\": f\"{err_p:.1f}%\"\n",
    "        })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining Finished in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# =================================\n",
    "# 7. Validation\n",
    "# =================================\n",
    "print(\"\\n=== Validation on Unseen Grid ===\")\n",
    "n_val = 200\n",
    "x_v = tf.cast(tf.linspace(-1.0, 1.0, n_val), DTYPE)\n",
    "y_v = tf.cast(tf.linspace(-1.0, 1.0, n_val), DTYPE)\n",
    "X_val, Y_val = tf.meshgrid(x_v, y_v)\n",
    "x_val_flat = tf.reshape(X_val, [-1, 1])\n",
    "y_val_flat = tf.reshape(Y_val, [-1, 1])\n",
    "\n",
    "def get_residuals(model, x, y):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([x, y])\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            out = model(tf.concat([x, y], axis=1))\n",
    "            phi, T_raw = out[:, 0:1], out[:, 1:2]\n",
    "            T = T_raw * hard_constraint_T(x, y)\n",
    "        gp_x, gp_y = tape1.gradient(phi, x), tape1.gradient(phi, y)\n",
    "        gt_x, gt_y = tape1.gradient(T, x), tape1.gradient(T, y)\n",
    "    g2p_xx = tape2.gradient(gp_x, x)\n",
    "    g2p_yy = tape2.gradient(gp_y, y)\n",
    "    g2t_xx = tape2.gradient(gt_x, x)\n",
    "    g2t_yy = tape2.gradient(gt_y, y)\n",
    "    \n",
    "    res_e = SIGMA_ELEC * (g2p_xx + g2p_yy)\n",
    "    J2 = (SIGMA_ELEC*gp_x)**2 + (SIGMA_ELEC*gp_y)**2\n",
    "    Q = (1.0/SIGMA_ELEC)*J2 + get_chip_layout_heat(x, y)\n",
    "    res_t = K_THERM * (g2t_xx + g2t_yy) + Q\n",
    "    return res_e, res_t\n",
    "\n",
    "res_e_val, res_t_val = get_residuals(model, x_val_flat, y_val_flat)\n",
    "mae_t = tf.reduce_mean(tf.abs(res_t_val))\n",
    "print(f\"Validation Thermal PDE Error: Mean={mae_t:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7790c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 8. Advanced Visualization\n",
    "# =================================\n",
    "print(\"Generating Advanced Visualization Plots...\")\n",
    "\n",
    "# --- 準備繪圖資料 ---\n",
    "n_grid = 400\n",
    "x_vals = np.linspace(-1, 1, n_grid)\n",
    "y_vals = np.linspace(-1, 1, n_grid)\n",
    "# 產生 2D 網格 (400, 400)\n",
    "X_grid, Y_grid = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# 準備給神經網路預測用的扁平輸入 (160000, 1)\n",
    "x_flat_np = X_grid.flatten()[:, None]\n",
    "y_flat_np = Y_grid.flatten()[:, None]\n",
    "x_tf = tf.cast(x_flat_np, DTYPE)\n",
    "y_tf = tf.cast(y_flat_np, DTYPE)\n",
    "\n",
    "# 1. 預測基礎物理量 (phi, T)\n",
    "# model 需要扁平輸入 (N, 2)\n",
    "out = model(tf.concat([x_tf, y_tf], axis=1))\n",
    "phi_pred_flat = out[:, 0].numpy()\n",
    "T_raw_flat = out[:, 1].numpy()\n",
    "\n",
    "# 應用 Hard Constraint\n",
    "dist_flat = (1 - x_flat_np**2) * (1 - y_flat_np**2)\n",
    "T_pred_flat = T_raw_flat * dist_flat\n",
    "\n",
    "# 重塑回 (400, 400)\n",
    "phi_grid = phi_pred_flat.reshape(n_grid, n_grid)\n",
    "T_grid = T_pred_flat.reshape(n_grid, n_grid)\n",
    "\n",
    "# 2. 計算衍生物理量\n",
    "# 計算梯度\n",
    "dy = dx = 2.0 / (n_grid - 1)\n",
    "grad_phi_y, grad_phi_x = np.gradient(phi_grid, dy, dx)\n",
    "grad_T_y, grad_T_x = np.gradient(T_grid, dy, dx)\n",
    "\n",
    "# 電流密度 J\n",
    "Jx_grid = -SIGMA_ELEC * grad_phi_x\n",
    "Jy_grid = -SIGMA_ELEC * grad_phi_y\n",
    "J_mag_grid = np.sqrt(Jx_grid**2 + Jy_grid**2)\n",
    "\n",
    "# 焦耳熱 Q_joule\n",
    "Q_joule_grid = (1.0 / SIGMA_ELEC) * J_mag_grid**2\n",
    "\n",
    "# --- [關鍵修正] 計算邏輯熱 Q_logic ---\n",
    "# 直接傳入 (400, 400) 的網格，不要傳入扁平向量\n",
    "# 這樣 TF 會直接輸出 (400, 400)，不需要 reshape，也不會發生廣播錯誤\n",
    "Q_logic_tf = get_chip_layout_heat(X_grid, Y_grid) \n",
    "Q_logic_grid = Q_logic_tf.numpy() # 確保轉回 numpy\n",
    "\n",
    "# 總熱源\n",
    "Q_total_grid = Q_joule_grid + Q_logic_grid\n",
    "\n",
    "# 熱通量 q\n",
    "qx_grid = -K_THERM * grad_T_x\n",
    "qy_grid = -K_THERM * grad_T_y\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Figure 1: Training Diagnostics\n",
    "# -------------------------------------------------\n",
    "fig1, ax1 = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs_idx = np.arange(len(history['total'])) * 10 # 假設每 10 epoch 存一次\n",
    "\n",
    "# Loss History\n",
    "ax1[0].semilogy(epochs_idx, history['total'], 'k-', label='Total', linewidth=2)\n",
    "ax1[0].semilogy(epochs_idx, history['elec'], 'b--', label='Elec')\n",
    "ax1[0].semilogy(epochs_idx, history['therm'], 'r--', label='Therm')\n",
    "ax1[0].semilogy(epochs_idx, history['glob'], 'g:', label='Global')\n",
    "ax1[0].semilogy(epochs_idx, history['bc'], 'c:', label='BC')\n",
    "ax1[0].axvline(x=EPOCHS_PHASE_1, color='gray', linestyle='--')\n",
    "ax1[0].axvline(x=EPOCHS_PHASE_1+EPOCHS_PHASE_2, color='gray', linestyle='--')\n",
    "ax1[0].set_xlabel('Epochs')\n",
    "ax1[0].set_ylabel('Loss (Log Scale)')\n",
    "ax1[0].set_title('Training Diagnostics: Loss History')\n",
    "ax1[0].legend()\n",
    "ax1[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Conservation Check\n",
    "ax1[1].plot(epochs_idx, history['gen_val'], 'r-', label='Total Heat Gen')\n",
    "ax1[1].plot(epochs_idx, history['flux_val'], 'b--', label='Total Flux Out')\n",
    "ax1[1].axvline(x=EPOCHS_PHASE_1, color='gray', linestyle='--')\n",
    "ax1[1].axvline(x=EPOCHS_PHASE_1+EPOCHS_PHASE_2, color='gray', linestyle='--')\n",
    "ax1[1].set_xlabel('Epochs')\n",
    "ax1[1].set_ylabel('Power (W)')\n",
    "ax1[1].set_title('Global Energy Balance Check')\n",
    "ax1[1].legend()\n",
    "ax1[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1_training_diagnostics.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close() # 釋放記憶體\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Figure 2: Multi-Physics Analysis\n",
    "# -------------------------------------------------\n",
    "fig2, ax2 = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# C. Potential & Current\n",
    "c1 = ax2[0].contourf(X_grid, Y_grid, phi_grid, 50, cmap='plasma')\n",
    "plt.colorbar(c1, ax=ax2[0], label='Potential (V)')\n",
    "# Streamplot 密度調整\n",
    "ax2[0].streamplot(X_grid, Y_grid, Jx_grid, Jy_grid, color='white', linewidth=0.8, density=1.0, arrowsize=1.0)\n",
    "ax2[0].set_title('C. Electric Potential & Current Flow (J)')\n",
    "ax2[0].set_aspect('equal')\n",
    "\n",
    "# D. Total Heat Source\n",
    "c2 = ax2[1].contourf(X_grid, Y_grid, Q_total_grid, 50, cmap='inferno')\n",
    "plt.colorbar(c2, ax=ax2[1], label='W/m^3')\n",
    "ax2[1].set_title('D. Total Heat Source (Logic + Joule)')\n",
    "ax2[1].set_aspect('equal')\n",
    "\n",
    "# E. Temp & Heat Flux\n",
    "c3 = ax2[2].contourf(X_grid, Y_grid, T_grid, 50, cmap='turbo')\n",
    "plt.colorbar(c3, ax=ax2[2], label='Temperature (K)')\n",
    "# Quiver 採樣調整\n",
    "skip = 25\n",
    "ax2[2].quiver(X_grid[::skip, ::skip], Y_grid[::skip, ::skip], \n",
    "              qx_grid[::skip, ::skip], qy_grid[::skip, ::skip], \n",
    "              color='black', scale=500, width=0.005)\n",
    "ax2[2].set_title('E. Temperature & Heat Flux Vectors (q)')\n",
    "ax2[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('2_multiphysics_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Figure 3: 1D Slice Analysis\n",
    "# -------------------------------------------------\n",
    "mid_idx = n_grid // 2\n",
    "x_slice = X_grid[mid_idx, :]\n",
    "phi_slice = phi_grid[mid_idx, :]\n",
    "T_slice = T_grid[mid_idx, :]\n",
    "J_mag_slice = J_mag_grid[mid_idx, :]\n",
    "Q_total_slice = Q_total_grid[mid_idx, :]\n",
    "\n",
    "fig3, ax3 = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F. Electrical Slice\n",
    "ax3[0].plot(x_slice, phi_slice, 'b-', label='Potential', linewidth=2)\n",
    "ax3[0].set_xlabel('x (at y=0)')\n",
    "ax3[0].set_ylabel('Potential (V)')\n",
    "ax3[0].set_title('F. 1D Slice: Electrical (y=0)')\n",
    "ax3[0].grid(True)\n",
    "ax3_twin = ax3[0].twinx()\n",
    "ax3_twin.plot(x_slice, J_mag_slice, 'r--', label='|J|')\n",
    "ax3_twin.set_ylabel('|J|', color='r')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='r')\n",
    "# 合併圖例\n",
    "l1, lab1 = ax3[0].get_legend_handles_labels()\n",
    "l2, lab2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3[0].legend(l1+l2, lab1+lab2, loc='upper right')\n",
    "\n",
    "# G. Thermal Slice\n",
    "ax3[1].plot(x_slice, T_slice, 'k-', label='Temp', linewidth=2)\n",
    "ax3[1].set_xlabel('x (at y=0)')\n",
    "ax3[1].set_ylabel('Temperature (K)')\n",
    "ax3[1].set_title('G. 1D Slice: Thermal (y=0)')\n",
    "ax3[1].grid(True)\n",
    "ax3_twin2 = ax3[1].twinx()\n",
    "ax3_twin2.plot(x_slice, Q_total_slice, 'm:', label='Q Source')\n",
    "ax3_twin2.set_ylabel('Heat Source Q', color='m')\n",
    "ax3_twin2.tick_params(axis='y', labelcolor='m')\n",
    "# 合併圖例\n",
    "l3, lab3 = ax3[1].get_legend_handles_labels()\n",
    "l4, lab4 = ax3_twin2.get_legend_handles_labels()\n",
    "ax3[1].legend(l3+l4, lab3+lab4, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('3_quantitative_slices.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"All plots saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
